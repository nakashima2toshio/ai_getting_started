{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# モデルの実装のまとめと学習の導入\n",
    "本単元は、総まとめに当たる単元になるので重要事項を再度確認しよう。\n",
    "\n",
    "> 本講座のタスク\n",
    "* タスクの内容：画像に写っている１文字の数字を何か判定する\n",
    "    * 用意したデータ：60,000枚の手書き数字が写ったラベル付きの画像データ(MNISTデータセット)\n",
    "    * 目的：精度を重視する\n",
    "\n",
    "> 機械学習の流れ\n",
    "1. ~アルゴリズムの選択~：ディープラーニングを選択\n",
    "2. ~前処理~：MNISTデータセットには必要なし\n",
    "3. モデルの学習\n",
    "4. 評価・検証\n",
    "\n",
    "この単元では**MLPモデルを学習させる方法**をメインに「3.モデルの学習」,「4.評価・検証」の実装をしていくぞ！\n",
    "具体的には、モデルの学習に使う`train()`関数と、評価・検証に使う`test()`関数の実装だ。\n",
    "\n",
    "## この単元の目標\n",
    "- MLPモデルを学習できるようになろう\n",
    "- MLPモデルをテストできるようになろう"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# モデルの実装のまとめと学習の導入\n",
    "本単元は、総まとめに当たる単元になるので重要事項を再度確認しよう。\n",
    "\n",
    "> 本講座のタスク\n",
    "* タスクの内容：画像に写っている１文字の数字を何か判定する\n",
    "    * 用意したデータ：60,000枚の手書き数字が写ったラベル付きの画像データ(MNISTデータセット)\n",
    "    * 目的：精度を重視する\n",
    "\n",
    "> 機械学習の流れ\n",
    "1. ~アルゴリズムの選択~：ディープラーニングを選択\n",
    "2. ~前処理~：MNISTデータセットには必要なし\n",
    "3. モデルの学習\n",
    "4. 評価・検証\n",
    "\n",
    "この単元では**MLPモデルを学習させる方法**をメインに「3.モデルの学習」,「4.評価・検証」の実装をしていくぞ！\n",
    "具体的には、モデルの学習に使う`train()`関数と、評価・検証に使う`test()`関数の実装だ。\n",
    "\n",
    "## この単元の目標\n",
    "- MLPモデルを学習できるようになろう\n",
    "- MLPモデルをテストできるようになろう"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. 準備"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZJTRwduamDI",
    "ExecuteTime": {
     "end_time": "2023-06-05T23:41:48.780532Z",
     "start_time": "2023-06-05T23:41:48.775370Z"
    }
   },
   "outputs": [],
   "source": [
    "# 本章で使うモジュールのインポート\n",
    "from torch import utils\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqQh5xZsPPPY"
   },
   "source": [
    "### 【MNISTデータセット】\n",
    "まずは、学習に使うデータセットを準備しよう。(MNISTについて忘れた人は単元1を確認しよう)  \n",
    "\n",
    "pytorchには、データセットがライブラリとしていくつか用意（実際にはダウンロードする関数）されており、MNISTもその1つだ。  \n",
    "\n",
    "以下のコードを実行すれば、\n",
    "* データセットの用意(ダウンロード)\n",
    "* 過学習回避のため、用意したデータセットを学習用と検証用に分ける  \n",
    "    * 下記のプログラムでは最初から別れたデータをダウンロードしている\n",
    "* ミニバッチごとにデータを纏める  \n",
    "    * バッチサイズは100、ランダムにバッチデータを決めることで効果的な学習になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4-uAOWgPjbh",
    "ExecuteTime": {
     "end_time": "2023-06-05T23:41:53.923572Z",
     "start_time": "2023-06-05T23:41:53.895284Z"
    }
   },
   "outputs": [],
   "source": [
    "## こうやってダウンロードして使うことができるよ\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())  # 学習用データセット\n",
    "train_loader = utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)  # ミニバッチごとにデータを纏める(学習時にはshuffle=True)\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())  # 検証用データセット\n",
    "test_loader = utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)  # ミニバッチごとにデータを纏める(学習時にはshuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"MPS is available\")\n",
    "else:\n",
    "    print(\"MPS is not available\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T18:42:50.451394Z",
     "start_time": "2023-06-05T18:42:50.442443Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MNIST データは、次の4つのファイルで構成されます。 役割ごとにファイルが分かれています。\n",
    "\n",
    "train-images-idx3-ubyte: 学習用の画像セット\n",
    "train-labels-idx1-ubyte: 学習用のラベルセット\n",
    "t10k-images-idx3-ubyte: 検証用の画像セット\n",
    "t10k-labels-idx1-ubyte: 検証用のラベルセット\n",
    "\n",
    "MNIST のファイルをデータをダウンロードすると JPEG などの画像が入っているのかと思いきや、予想とは違い、次のような仕様のデータファイルが含まれていました。 1ファイルの中に複数の画像やラベルのデータが入っており、ファイルを読込む際にはその仕様に則る必要があります。 ただし、後述する各種ライブラリのクラス・関数を利用すれば簡単にデータを読込むことができます。\n",
    "\n",
    "ラベルデータ (train-labels-idx1-ubyte / t10k-labels-idx1-ubyte)\n",
    "　ラベルデータが保存されたファイル(train-labels-idx1-ubyte / t10k-labels-idx1-ubyte)は、次のような仕様になっています。\n",
    "\n",
    "offset\ttype\tvalue\tdescription\n",
    "0000\t32 bit integer\t0x00000801(2049)\t識別子(定数)\n",
    "0004\t32 bit integer\t60000 or 10000\tラベルデータの数\n",
    "0008\tunsigned byte\t0 ～ 9\t1つ目のデータのラベル\n",
    "0009\tunsigned byte\t0 ～ 9\t2つ目のデータのラベル\n",
    "....\t....\t....\t....\n",
    "xxxx\tunsigned byte\t0 ～ 9\t最後のデータのラベル\n",
    "ラベルは、画像が何の数字を表すかの 0 から 9 までの数値です。\n",
    "\n",
    "画像データ (train-images-idx3-ubyte / t10k-images-idx3-ubyte)\n",
    "　画像データが保存されたファイル(train-images-idx3-ubyte / t10k-images-idx3-ubyte)は、次のような仕様になっています。\n",
    "\n",
    "offset\ttype\tvalue\tdescription\n",
    "0000\t32 bit integer\t0x00000803(2051)\t識別子(定数)\n",
    "0004\t32 bit integer\t60000\t画像データの数\n",
    "0008\t32 bit integer\t28\t1画像あたりのデータ行数\n",
    "0012\t32 bit integer\t28\t1画像あたりのデータ列数\n",
    "0016\tunsigned byte\t0 ～ 255\t1つめの画像の1ピクセル目の値\n",
    "0017\tunsigned byte\t0 ～ 255\t1つめの画像の2ピクセル目の値\n",
    "....\t....\t....\t....\n",
    "xxxx\tunsigned byte\t0 ～ 255\t最後の画像の784ピクセル目の値\n",
    "ピクセルの値は、0 から 255 までの値で、0 が白を, 255 が黒を表します。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataloader.DataLoader at 0x145e45610>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T18:42:51.030159Z",
     "start_time": "2023-06-05T18:42:51.025404Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XowdNPCCYfGF"
   },
   "source": [
    "### 【モデルの定義】\n",
    "次に、学習を行うためのモデルを準備しよう。  \n",
    "行うタスクは、**「28*28画素の白黒画像を10クラス分類する」**事なので、  \n",
    "入力ノードの数は784（=28*28）個で、出力ノードの数は10個で良いだろう。\n",
    "\n",
    "よって、以下の条件のMLPモデルをクラスとして定義する。  \n",
    "- 「入力層、中間層、出力層」のノードの数が「784（=28×28）、512、10」\n",
    "- 中間層の出力に活性化関数`relu()`を適用\n",
    "- 損失関数は`MSELoss()`\n",
    "- 最適化関数は`Adam()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRPWplc-ZGYF",
    "ExecuteTime": {
     "end_time": "2023-06-05T23:42:15.317559Z",
     "start_time": "2023-06-05T23:42:15.312851Z"
    }
   },
   "outputs": [],
   "source": [
    "class mlp_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYWI7-a2V7yq"
   },
   "source": [
    "## 1. train()関数の実装\n",
    "それでは、**`train()`関数の実装**を行っていこう。\n",
    "実装にあたり、気をつける点が2点あるので\n",
    "- 2次元の画像であるテンソルを1次元のテンソルに変換する。\n",
    "- 損失を求めるために正解データをone-hotベクトル化する。\n",
    "\n",
    "MLPはその構造上、**2次元以上である画像データをその形状のまま入力として受け取る事ができない。**  \n",
    "よって、画像を行や列で区切るなどして**1次元のテンソルに変換してからモデルに入力**する。  \n",
    "今回は、形状変形のために`reshape()`を使おう。\n",
    "\n",
    "また、モデルの出力は**各クラスの予測確率に相関したベクトル**であるのに対し、**正解ラベルはクラス名（0~9の値）**であるため、  \n",
    "そのままではMSEを計算する事ができない。  \n",
    "そこで、正解ラベルを「one-hotベクトルに変換」することで、モデルの予測結果との損失計算を可能にする。  \n",
    "【one-hotベクトル化の例】\n",
    "- 正解ラベル「1」 → `[0,1,0,0,0,0,0,0,0,0]` （=「1」である確率が1の確率ベクトル）\n",
    "- 正解ラベル「7」 → `[0,0,0,0,0,0,0,1,0,0]` （=「7」である確率が1の確率ベクトル）\n",
    "\n",
    "以上のことを踏まえて、例題を見ながら`train()`関数の実装方法を確認しよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CB7fXVCnwRnd"
   },
   "source": [
    "【例題】 `train()`関数を実装する。 \n",
    "`train()`関数では、引数に`model`（学習するモデル）と`train_loader`（学習データローダ）を受け取り、  \n",
    "`train_loader`を「ミニバッチ=ある程度の数のデータのまとまり」ごとにループさせて学習を行う。  \n",
    "この1ループは、次の流れで進む。\n",
    "1. バッチごとのデータをモデルへ順伝播させる\n",
    "2. 正解ラベルと比較して損失を計算する\n",
    "3. 損失から最適化を行う\n",
    "4. 次のバッチへ\n",
    "\n",
    "これに加えて**「画像データの1次元化」**と**「正解ラベルのone-hotベクトル化」**を忘れないようにしよう。  \n",
    "`torch.eye(クラス数)[バッチごとのデータ]`と記述すると、`バッチごとのデータ`の1要素に対してone-hotベクトル化することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "22EuWFr4WQGR",
    "ExecuteTime": {
     "end_time": "2023-06-05T18:43:03.432274Z",
     "start_time": "2023-06-05T18:43:03.428183Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    # 今は学習時であることを明示するコード\n",
    "    model.train()\n",
    "    # ミニバッチごとにループさせる,train_loaderの中身を出し切ったら1エポックとなる\n",
    "    for batch_imgs, batch_labels in train_loader:\n",
    "        batch_imgs = batch_imgs.reshape(-1, 28*28*1).to(device)  # 画像データを1次元に変換\n",
    "        labels = torch.eye(10)[batch_labels].to(device)  # 正解ラベルをone-hotベクトルへ変換\n",
    "\n",
    "        outputs = model(batch_imgs)  # 順伝播\n",
    "        model.optimizer.zero_grad()  # 勾配を初期化（前回のループ時の勾配を削除）\n",
    "        loss = model.criterion(outputs, labels)  # 損失を計算\n",
    "        loss.backward()  # 逆伝播で勾配を計算\n",
    "        model.optimizer.step()  # 最適化\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTrJa_Bn_eOn"
   },
   "source": [
    "以上のコードが、モデルの学習を行うプログラムの核にあたる。  \n",
    "\n",
    "しかし、このままでは学習進捗の確認ができないので、「正答率」と「損失」を出力できるように、以下のように追記する。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8euG-MK98LR",
    "ExecuteTime": {
     "end_time": "2023-06-05T18:53:29.917352Z",
     "start_time": "2023-06-05T18:53:29.638703Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, train_loader):\n",
    "    # 今は学習時であることを明示するコード\n",
    "    model.train()\n",
    "\n",
    "    ### 追記部分1 ###\n",
    "    # 正しい予測数、損失の合計、全体のデータ数を数えるカウンターの0初期化\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    total_data_len = 0\n",
    "    ### ###\n",
    "\n",
    "    # ミニバッチごとにループさせる,train_loaderの中身を出し切ったら1エポックとなる\n",
    "    for batch_imgs, batch_labels in train_loader:\n",
    "\n",
    "        batch_imgs = batch_imgs.reshape(-1, 28*28*1).to(device)   # 画像データを1次元に変換\n",
    "        labels = torch.eye(10)[batch_labels].to(device)   # 正解ラベルをone-hotベクトルへ変換\n",
    "        plt.imshow(batch_imgs)\n",
    "        plt.show()\n",
    "\n",
    "        outputs = model(batch_imgs)  # 順伝播\n",
    "        model.optimizer.zero_grad()  # 勾配を初期化（前回のループ時の勾配を削除）\n",
    "        loss = model.criterion(outputs, labels)  # 損失を計算\n",
    "        loss.backward()  # 逆伝播で勾配を計算\n",
    "        model.optimizer.step()  # 最適化\n",
    "       \n",
    "        ### 追記部分2 ###\n",
    "        # ミニバッチごとの正答率と損失を求める\n",
    "        _, pred_labels = torch.max(outputs, axis=1)  # outputsから必要な情報(予測したラベル)のみを取り出す。\n",
    "        batch_size = len(batch_labels)  # バッチサイズの確認\n",
    "        for i in range(batch_size):  # データ一つずつループ,ミニバッチの中身出しきるまで\n",
    "            total_data_len += 1  # 全データ数を集計\n",
    "            if pred_labels[i] == batch_labels[i]:\n",
    "                total_correct += 1 # 正解のデータ数を集計\n",
    "        total_loss += loss.item()  # 全損失の合計\n",
    "\n",
    "    # 今回のエポックの正答率と損失を求める\n",
    "    accuracy = total_correct/total_data_len*100  # 予測精度の算出\n",
    "    loss = total_loss/total_data_len  # 損失の平均の算出\n",
    "    return accuracy, loss\n",
    "    ### ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10221,
     "status": "ok",
     "timestamp": 1593136130415,
     "user": {
      "displayName": "Kurusu Yuugo",
      "photoUrl": "",
      "userId": "00117977046560544733"
     },
     "user_tz": -540
    },
    "id": "wDSSBkxZZPu0",
    "outputId": "7f529153-9c33-4a2d-ecc4-ba4349d65cdd",
    "ExecuteTime": {
     "end_time": "2023-06-05T18:53:47.766431Z",
     "start_time": "2023-06-05T18:53:45.977649Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m mlp_net()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 学習させ、その結果を表示する\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m acc, loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m正答率: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00macc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, 損失: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[37], line 19\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader)\u001B[0m\n\u001B[1;32m     17\u001B[0m batch_imgs \u001B[38;5;241m=\u001B[39m batch_imgs\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m28\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m28\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)   \u001B[38;5;66;03m# 画像データを1次元に変換\u001B[39;00m\n\u001B[1;32m     18\u001B[0m labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meye(\u001B[38;5;241m10\u001B[39m)[batch_labels]\u001B[38;5;241m.\u001B[39mto(device)   \u001B[38;5;66;03m# 正解ラベルをone-hotベクトルへ変換\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_imgs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     22\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(batch_imgs)  \u001B[38;5;66;03m# 順伝播\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ai_getting_start/venv/lib/python3.11/site-packages/matplotlib/pyplot.py:2695\u001B[0m, in \u001B[0;36mimshow\u001B[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001B[0m\n\u001B[1;32m   2689\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mimshow)\n\u001B[1;32m   2690\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mimshow\u001B[39m(\n\u001B[1;32m   2691\u001B[0m         X, cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m, aspect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, interpolation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   2692\u001B[0m         alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, vmin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, vmax\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, origin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, extent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   2693\u001B[0m         interpolation_stage\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, filternorm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, filterrad\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4.0\u001B[39m,\n\u001B[1;32m   2694\u001B[0m         resample\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, url\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 2695\u001B[0m     __ret \u001B[38;5;241m=\u001B[39m \u001B[43mgca\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimshow\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2696\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcmap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcmap\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnorm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnorm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maspect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maspect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2697\u001B[0m \u001B[43m        \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvmin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2698\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvmax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvmax\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morigin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morigin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2699\u001B[0m \u001B[43m        \u001B[49m\u001B[43minterpolation_stage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolation_stage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2700\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilternorm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilternorm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilterrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilterrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2701\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2702\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2703\u001B[0m     sci(__ret)\n\u001B[1;32m   2704\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m __ret\n",
      "File \u001B[0;32m~/PycharmProjects/ai_getting_start/venv/lib/python3.11/site-packages/matplotlib/__init__.py:1442\u001B[0m, in \u001B[0;36m_preprocess_data.<locals>.inner\u001B[0;34m(ax, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1439\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m   1440\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(ax, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1441\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1442\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msanitize_sequence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1444\u001B[0m     bound \u001B[38;5;241m=\u001B[39m new_sig\u001B[38;5;241m.\u001B[39mbind(ax, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1445\u001B[0m     auto_label \u001B[38;5;241m=\u001B[39m (bound\u001B[38;5;241m.\u001B[39marguments\u001B[38;5;241m.\u001B[39mget(label_namer)\n\u001B[1;32m   1446\u001B[0m                   \u001B[38;5;129;01mor\u001B[39;00m bound\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(label_namer))\n",
      "File \u001B[0;32m~/PycharmProjects/ai_getting_start/venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5665\u001B[0m, in \u001B[0;36mAxes.imshow\u001B[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001B[0m\n\u001B[1;32m   5657\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_aspect(aspect)\n\u001B[1;32m   5658\u001B[0m im \u001B[38;5;241m=\u001B[39m mimage\u001B[38;5;241m.\u001B[39mAxesImage(\u001B[38;5;28mself\u001B[39m, cmap\u001B[38;5;241m=\u001B[39mcmap, norm\u001B[38;5;241m=\u001B[39mnorm,\n\u001B[1;32m   5659\u001B[0m                       interpolation\u001B[38;5;241m=\u001B[39minterpolation, origin\u001B[38;5;241m=\u001B[39morigin,\n\u001B[1;32m   5660\u001B[0m                       extent\u001B[38;5;241m=\u001B[39mextent, filternorm\u001B[38;5;241m=\u001B[39mfilternorm,\n\u001B[1;32m   5661\u001B[0m                       filterrad\u001B[38;5;241m=\u001B[39mfilterrad, resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[1;32m   5662\u001B[0m                       interpolation_stage\u001B[38;5;241m=\u001B[39minterpolation_stage,\n\u001B[1;32m   5663\u001B[0m                       \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 5665\u001B[0m \u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5666\u001B[0m im\u001B[38;5;241m.\u001B[39mset_alpha(alpha)\n\u001B[1;32m   5667\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im\u001B[38;5;241m.\u001B[39mget_clip_path() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5668\u001B[0m     \u001B[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ai_getting_start/venv/lib/python3.11/site-packages/matplotlib/image.py:697\u001B[0m, in \u001B[0;36m_ImageBase.set_data\u001B[0;34m(self, A)\u001B[0m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(A, PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage):\n\u001B[1;32m    696\u001B[0m     A \u001B[38;5;241m=\u001B[39m pil_to_array(A)  \u001B[38;5;66;03m# Needed e.g. to apply png palette.\u001B[39;00m\n\u001B[0;32m--> 697\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_A \u001B[38;5;241m=\u001B[39m \u001B[43mcbook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msafe_masked_invalid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    699\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_A\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m np\u001B[38;5;241m.\u001B[39muint8 \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    700\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39mcan_cast(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_A\u001B[38;5;241m.\u001B[39mdtype, \u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msame_kind\u001B[39m\u001B[38;5;124m\"\u001B[39m)):\n\u001B[1;32m    701\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage data of dtype \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m cannot be converted to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    702\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_A\u001B[38;5;241m.\u001B[39mdtype))\n",
      "File \u001B[0;32m~/PycharmProjects/ai_getting_start/venv/lib/python3.11/site-packages/matplotlib/cbook/__init__.py:709\u001B[0m, in \u001B[0;36msafe_masked_invalid\u001B[0;34m(x, copy)\u001B[0m\n\u001B[1;32m    708\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msafe_masked_invalid\u001B[39m(x, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 709\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubok\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m x\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39misnative:\n\u001B[1;32m    711\u001B[0m         \u001B[38;5;66;03m# If we have already made a copy, do the byteswap in place, else make a\u001B[39;00m\n\u001B[1;32m    712\u001B[0m         \u001B[38;5;66;03m# copy with the byte order swapped.\u001B[39;00m\n\u001B[1;32m    713\u001B[0m         x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mbyteswap(inplace\u001B[38;5;241m=\u001B[39mcopy)\u001B[38;5;241m.\u001B[39mnewbyteorder(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# Swap to native order.\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ai_getting_start/venv/lib/python3.11/site-packages/torch/_tensor.py:970\u001B[0m, in \u001B[0;36mTensor.__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor\u001B[38;5;241m.\u001B[39m__array__, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m    969\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    972\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mTypeError\u001B[0m: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# モデルを宣言する\n",
    "model = mlp_net().to(device)\n",
    "\n",
    "# 学習させ、その結果を表示する\n",
    "acc, loss = train(model, train_loader)\n",
    "print(f'正答率: {acc}, 損失: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCP1JwtARCI_"
   },
   "source": [
    "- ```\n",
    "正答率： {95.0前後}, 損失: {0.0002前後}\n",
    "```\n",
    "と表示されていれば成功だ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRTxU6U9QPgR"
   },
   "source": [
    "【問題】 新しいMLPモデル`mlp_net_2()`をクラスとして定義して、学習、結果を例題と同様に表示しよう。  \n",
    "ただし、以下の条件にあるようにハイパーパラメータを指定すること。\n",
    "- 宣言する`mlp_net_2()`のインスタンス名（変数名）は`model_2`とすること\n",
    "- 中間層のノードの数を`256`にする\n",
    "- `optim.Adam()`の引数`lr`に`0.01`を指定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfXBJ_IGRTpv",
    "ExecuteTime": {
     "end_time": "2023-06-05T18:44:41.552746Z",
     "start_time": "2023-06-05T18:44:41.546981Z"
    }
   },
   "outputs": [],
   "source": [
    "class mlp_net_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfXBJ_IGRTpv",
    "ExecuteTime": {
     "end_time": "2023-06-05T18:45:22.619697Z",
     "start_time": "2023-06-05T18:45:08.002984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正答率: 89.19, 損失: 0.0002787030518831064\n"
     ]
    }
   ],
   "source": [
    "# モデルを宣言する\n",
    "model_2 = mlp_net_2().to(device)\n",
    "\n",
    "# 学習させ、その結果を表示する\n",
    "acc, loss = train(model_2, train_loader)\n",
    "print(f'正答率: {acc}, 損失: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正答率: 94.39999999999999, 損失: 0.00014445836887850115\n",
      "正答率: 94.53166666666667, 損失: 0.00014237657270083824\n",
      "正答率: 94.47833333333332, 損失: 0.00014311055287253113\n"
     ]
    }
   ],
   "source": [
    "# 学習させ、その結果を表示する\n",
    "acc, loss = train(model_2, train_loader)\n",
    "print(f'正答率: {acc}, 損失: {loss}')\n",
    "\n",
    "acc, loss = train(model_2, train_loader)\n",
    "print(f'正答率: {acc}, 損失: {loss}')\n",
    "\n",
    "acc, loss = train(model_2, train_loader)\n",
    "print(f'正答率: {acc}, 損失: {loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T18:48:22.123902Z",
     "start_time": "2023-06-05T18:47:38.332119Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PqywSvzSEvM"
   },
   "source": [
    "- ```\n",
    "正答率： {90.0前後}, 損失: {0.00025前後}\n",
    "```\n",
    "と表示されていれば成功だ。\n",
    "- 恐らく、例題よりも正答率は下がり、損失は大きくなったのではないだろうか。  \n",
    "このように、ハイパーパラメータは学習に大きく影響するということを覚えておこう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "foTqDLiCk2g5"
   },
   "source": [
    "## test()関数の実装\n",
    "次は、テストデータを使った評価を行う`test()`関数を実装する。  \n",
    "とは言っても、`train()`関数から「損失計算」や「最適化」の要素を取り除けば良いだけだ。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FP4c6sieZfOj",
    "ExecuteTime": {
     "end_time": "2023-06-05T17:42:15.188940Z",
     "start_time": "2023-06-05T17:42:15.185529Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    # モデルを評価モードにする\n",
    "    model.eval()\n",
    "    # 正しい予測数、全体のデータ数を数えるカウンターの0初期化\n",
    "    total_data_len = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch_imgs, batch_labels in data_loader:\n",
    "        outputs = model(batch_imgs.reshape(-1, 28*28*1))  # 順伝播（=予測）\n",
    "         \n",
    "        # ミニバッチごとの集計\n",
    "        _, pred_labels = torch.max(outputs, axis=1)  # outputsから必要な情報(予測したラベル)のみを取り出す。\n",
    "        batch_size = len(pred_labels)  # バッチサイズの確認\n",
    "        for i in range(batch_size):  # データ一つずつループ,ミニバッチの中身出しきるまで\n",
    "            total_data_len += 1  # 全データ数を集計\n",
    "            if pred_labels[i] == batch_labels[i]:\n",
    "                total_correct += 1 # 正解のデータ数を集計\n",
    "\n",
    "    # 1エポック分の集計\n",
    "    acc = 100.0 * total_correct/total_data_len  # 予測精度の算出\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHTpEKJBNK3p"
   },
   "source": [
    "【例題】 学習させた`mlp_net`と`test_loader`を使って、テストを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18965,
     "status": "ok",
     "timestamp": 1593136139179,
     "user": {
      "displayName": "Kurusu Yuugo",
      "photoUrl": "",
      "userId": "00117977046560544733"
     },
     "user_tz": -540
    },
    "id": "DDAwyPSSkQH-",
    "outputId": "3b9e2c1c-e2ea-4cd4-8b7e-f79514184bd1",
    "ExecuteTime": {
     "end_time": "2023-06-05T17:42:20.286337Z",
     "start_time": "2023-06-05T17:42:19.132661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.13\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(model, test_loader)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DH5RIZMhNefA"
   },
   "source": [
    "- 正答率はおよそ`95%`前後になるだろう。それであれば成功だ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIMlmcOzShRs"
   },
   "source": [
    "【問題】 学習させた`mlp_net_2`と`test_loader`を使って、テストを行ってみよう。テストデータの正答率を出力すれば良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20024,
     "status": "ok",
     "timestamp": 1593136140243,
     "user": {
      "displayName": "Kurusu Yuugo",
      "photoUrl": "",
      "userId": "00117977046560544733"
     },
     "user_tz": -540
    },
    "id": "RkUTYIgpShRt",
    "outputId": "5db893a4-996b-410c-c27f-f273fc249293",
    "ExecuteTime": {
     "end_time": "2023-05-29T20:04:05.043412Z",
     "start_time": "2023-05-29T20:04:03.756362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.39\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(model_2, test_loader)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trzhGmgsShRx"
   },
   "source": [
    "- 恐らく正答率は例題と同等かそれ以下になるだろう。それであれば成功だ。\n",
    "- ハイパーパラメータの影響によって、学習データ・テストデータともに予測精度が落ちている可能性が高い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1utcb96gbbe"
   },
   "source": [
    "## まとめ\n",
    "MLPモデルを実装し、テストした！\n",
    "\n",
    "Pytorchを使ってのプログラミングは理論より、簡単だったのではないだろうか？\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wq8H0sGT_Vci"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7_モデルの学習.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
